Q-learning is an off-policy Temporal Difference (TD) control algorithm that operates based on the action-value function. The action selection mechanism in this program follows an epsilon-greedy approach. With a probability of epsilon, the action is chosen randomly, while with a probability of 1 - epsilon, it is selected based on the maximum Q-value. Subsequently, the agent interacts with the environment to observe the next state and the corresponding reward. Finally, the Q-learning formula is updated with all the data collected during the environment interaction. This loop continues for each episode until it reaches the final step. As mentioned earlier, when the agent encounters an obstacle or reaches the goal, the 'Done' flag activates, and the entire process restarts.
