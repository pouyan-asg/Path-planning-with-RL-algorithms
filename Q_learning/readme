Q-learning is an off-policy TD control algorithm that works based on the action-value function. The action selection mechanism in this program is epsilon - greedy. With the probability of epsilon, the action is chosen randomly, and with a probability of 1 - epsilon, it is selected regarding maximum Q. After action selection, the agent moves in the environment to see the next state and reward. Finally, the Q-learning formula would be updated with all data collected in the environment. The mentioned loop repeats each episode until it reaches the final step. As declared before, whenever the agent reaches an obstacle or goal, the Done flag will active, and all the processes will restart.
