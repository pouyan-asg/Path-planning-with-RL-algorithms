TD method uses experience to solve the prediction problem. Given some experiences following a policy, this method updates its estimate V for the non-terminal states occurring in that experience. In this algorithm, the actions are selected regarding the policy. In each state, the agent randomly chooses an action regarding a simple policy. When going right hits an obstacle, the agent will choose other actions randomly, and this situation is the same for other directions. 
