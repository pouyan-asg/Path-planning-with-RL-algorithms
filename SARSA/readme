SARSA is an on-policy TD control method that has common features with Q-learning. The convergence properties of the Sarsa algorithm depend on the nature of the policyâ€™s dependence on Q. The general form of the Sarsa control algorithm is given in the box below. As it is understandable, the SARSA needs the next action to update the Q table, and hence it chooses the next action from the next state. The action selection mechanism is epsilon greedy, and all procedures are the same as Q-learning except for episodes in which SARSA requires more time to find the optimal path.
