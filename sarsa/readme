SARSA is an on-policy Temporal Difference (TD) control method that shares common features with Q-learning. The convergence properties of the SARSA algorithm depend on the nature of the policy's reliance on Q. As evident, SARSA necessitates the next action to update the Q table, and therefore, it selects the next action based on the next state. The action selection mechanism follows an epsilon-greedy approach, and all procedures are akin to Q-learning, with the exception of episodes during which SARSA requires more time to discover the optimal path.
