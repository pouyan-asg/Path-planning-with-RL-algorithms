SARSA is an on-policy Temporal Difference (TD) control method that shares common features with Q-learning. The convergence properties of the SARSA algorithm depend on the nature of the policy's reliance on Q. The general form of the SARSA control algorithm is provided in the box below. As evident, SARSA necessitates the next action to update the Q table, and therefore, it selects the next action based on the next state. The action selection mechanism follows an epsilon-greedy approach, and all procedures are akin to Q-learning, with the exception of episodes during which SARSA requires more time to discover the optimal path.
